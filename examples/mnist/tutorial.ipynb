{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "## Introduction\n",
    "\n",
    "NanoFed is a Python library designed to simplify the implementation of federated learning systems, offering out-of-the-box support for coordination, client-server communication, and model aggregation.\n",
    "\n",
    "In this tutorial, we guide you step-by-step through setting up a federated learning experiment using NanoFed. You will learn how to configure a federated server, manage clients, and utilize the built-in aggregation strategies to perform FL on an example dataset. This tutorial uses the MNIST dataset and a simple classification model, but NanoFed can work with any PyTorch-based classification model and dataset.\n",
    "\n",
    "First, make sure you have PyTorch and NanoFed installed:\n",
    "\n",
    "```bash\n",
    "pip install nanofed\n",
    "```\n",
    "\n",
    "## Step 1: Import Required Modules\n",
    "\n",
    "Start by importing the necessary modules. `load_mnist_data` and `MNISTModel` are provided as examples in the NanoFed library. You can replace these with any PyTorch `nn.Module` that performs classification and PyTorch `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from nanofed import (\n",
    "    Coordinator,\n",
    "    CoordinatorConfig,\n",
    "    FedAvgAggregator,\n",
    "    HTTPClient,\n",
    "    HTTPServer,\n",
    "    ModelManager,\n",
    "    coordinate,\n",
    "    TrainingConfig,\n",
    "    TorchTrainer\n",
    ")\n",
    "from nanofed.data import load_mnist_data\n",
    "from nanofed.models import MNISTModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preparing the Global Model\n",
    "\n",
    "Set up the global model and initialize the model manager.\n",
    "\n",
    "The **global model** is a shared model that all clients collaboratively train. At the beginning of the trianing process, the global model is initialized and distributed to all participating clients. Each client then trains this model locally on its private dataset and submits updates back to the server. The server aggregates these updates to refine the global model.\n",
    "\n",
    "In this step, we define the global model and set up a `ModelManager` to handle its versions and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory for outputs and checkpoints\n",
    "base_dir = Path(\"runs/\")\n",
    "\n",
    "# Initialize the global model\n",
    "model = MNISTModel()  # Any PyTorch classification model\n",
    "model_manager = ModelManager(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting up the Federated Server\n",
    "\n",
    "The server is the central communication hub in a FL setup. It facilitates interactions between the global model and participating clients. It is responsible for:\n",
    "1. **Distributing the Global Model**: Clients fetch the current state of the global model from the server.\n",
    "2. **Collecting Updates**: Cleints send their locally computed updates to the server after training on their private dataset.\n",
    "3. **Orchestrating Rounds**: The server manages the flow of training rounds.\n",
    "\n",
    "In NanoFed, the `HTTPServer` class implements these functionalities using an HTTP-based protocol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:28:04,875 - nanofed - \u001b[36mDEBUG\u001b[0m - Setting up routes for HTTP server.\n",
      "2024-12-11 21:28:04,876 - nanofed - \u001b[32mINFO\u001b[0m - Starting HTTP server...\n",
      "2024-12-11 21:28:04,876 - nanofed - \u001b[32mINFO\u001b[0m - HTTP server started on 0.0.0.0:8080\n"
     ]
    }
   ],
   "source": [
    "server = HTTPServer(\n",
    "    host=\"0.0.0.0\",\n",
    "    port=8080,\n",
    "    max_request_size=100 * 1024 * 1024,  # Limit the size of incoming requests. Useful for controlling the size of model updates sent by clients.\n",
    ")\n",
    "\n",
    "# Begin listening for client connections\n",
    "await server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configuring the Aggregator\n",
    "\n",
    "An **aggregator** is a server component that combines model updates from clients to form a new global model. The aggregation strategy determines how these updates are combined, which can significantly impact the learning process.\n",
    "\n",
    "### Default Aggregation Strategy: Federated Averaging\n",
    "\n",
    "As of NanoFed version **0.1.4**, the library supports the **Federated Averaging (FedAvg)** strategy through the `FedAvgAggregator` class. This strategy:\n",
    "1. Computes a weighted average of client model updates based on the number of samples each client processes.\n",
    "2. Aggregates metrics from clients, such as accuracy or loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the aggregator\n",
    "aggregator = FedAvgAggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from nanofed.core import ModelProtocol, ModelUpdate\n",
    "from nanofed.server import AggregationResult, BaseAggregator\n",
    "\n",
    "\n",
    "class CustomAggregator(BaseAggregator[ModelProtocol]):\n",
    "    def aggregate(self, model: ModelProtocol, updates: Sequence[ModelUpdate]) -> AggregationResult[ModelProtocol]:\n",
    "        # Custom aggregation logic\n",
    "        pass\n",
    "\n",
    "    def _compute_weights(self, updates: Sequence[ModelUpdate]) -> list[float]:\n",
    "        # Custom weighting logic\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Defining the Coordinator Configuration\n",
    "\n",
    "The **Coordinator** is a central component in a FL workflow. It manages the orchestration of training rounds, including scheduling, communication, and validation of client updates. Before creating the Coordinator, you need to define its configuration.\n",
    "\n",
    "### Coordinator Configuration\n",
    "\n",
    "The `CoordinatorConfig` class specifies the parameters that govern the FL process. These include the number of training rounds, client participation criteria, timeout durations, and directories storing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinator_config = CoordinatorConfig(\n",
    "    num_rounds=2,\n",
    "    min_clients=3,\n",
    "    min_completion_rate=0.5,\n",
    "    round_timeout=300,\n",
    "    base_dir=base_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Parameters\n",
    "1. `num_rounds`: Specifies the total number of training rounds.\n",
    "2. `min_clients`: Minimum number of clients required to participate in each round.\n",
    "3. `min_completion_rate`: Minimum fraction of total clients that must complete their training updates in a round.\n",
    "4. `round_timeout`: Maximum time (in seconds) to wait for client updates during a training round.\n",
    "5. `base_dir`: Base directory for storing data, including metrics, model weights, and configuration files.\n",
    "\n",
    "## Step 6: Initializing the Coordinator\n",
    "\n",
    "In this step, you'll create an instance of the `Coordinator` class using the previously configured components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:28:04,888 - coordinator.setup - \u001b[32mINFO\u001b[0m - Created directory: runs/metrics\n",
      "2024-12-11 21:28:04,889 - coordinator.setup - \u001b[32mINFO\u001b[0m - Created directory: runs/data\n",
      "2024-12-11 21:28:04,889 - coordinator.setup - \u001b[32mINFO\u001b[0m - Created directory: runs/models/configs\n",
      "2024-12-11 21:28:04,889 - coordinator.setup - \u001b[32mINFO\u001b[0m - Created directory: runs/models/models\n"
     ]
    }
   ],
   "source": [
    "coordinator = Coordinator(\n",
    "    model_manager=model_manager,\n",
    "    aggregator=aggregator,\n",
    "    server=server,\n",
    "    config=coordinator_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Implementing a Federeated Client\n",
    "\n",
    "In FL, **clients** are devices or nodes that perform local training on their data and send updates to the server. Each client operates independently.\n",
    "\n",
    "### Overview of a Federated Client's Workflow\n",
    "1. **Dataset Preparation**:\n",
    "    - Load the local dataset for the client, making sure it matches the expected input for the global model.\n",
    "2. **Training**:\n",
    "    - Train the model locally using the client's dataset.\n",
    "3. **Communication**:\n",
    "    - Fetch the global model from the server.\n",
    "    - Submit locally trained updates and metrics to the server.\n",
    "4. **Iteration**:\n",
    "    - Repeat the process for each training round until the server signals completion.\n",
    "\n",
    "### Client Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_client(client_id: str, coordinator: Coordinator, num_samples: int):\n",
    "    # Prepare the client's local dataset\n",
    "    # Use any PyTorch DataLoader.\n",
    "\n",
    "    # Use different subset sizes for each client to demonstrate FedAvg weighting.\n",
    "    subset_fraction = num_samples / 60000  # MNIST has 60,000 samples\n",
    "\n",
    "    train_loader = load_mnist_data(\n",
    "        data_dir=coordinator.data_dir,\n",
    "        batch_size=64,\n",
    "        train=True,\n",
    "        subset_fraction=subset_fraction\n",
    "    )\n",
    "\n",
    "    # Configure training hyperparameters\n",
    "    training_config = TrainingConfig(\n",
    "        epochs=2,\n",
    "        batch_size=256,\n",
    "        learning_rate=0.1,\n",
    "        device=\"cpu\",\n",
    "        log_interval=10,\n",
    "    )\n",
    "    trainer = TorchTrainer(training_config)\n",
    "\n",
    "    # Server URL for communication\n",
    "    server_url = coordinator.server.url\n",
    "\n",
    "    async with HTTPClient(server_url=server_url, client_id=client_id) as client:\n",
    "        while True:\n",
    "            try:\n",
    "                # Check if the server has completed training\n",
    "                if await client.check_server_status():\n",
    "                    break\n",
    "                \n",
    "                # Fetch the global model from the server\n",
    "                model_state, _ = await client.fetch_global_model()\n",
    "                model = MNISTModel()\n",
    "                model.load_state_dict(model_state) # Load global model parameters\n",
    "                model.to(training_config.device)\n",
    "\n",
    "                # Perform local training\n",
    "                optimizer = torch.optim.SGD(\n",
    "                    model.parameters(),\n",
    "                    lr=training_config.learning_rate\n",
    "                )\n",
    "                metrics = None\n",
    "                for epoch in range(training_config.epochs):\n",
    "                    metrics = trainer.train_epoch(\n",
    "                        model, train_loader, optimizer, epoch\n",
    "                    )\n",
    "\n",
    "                # Submit the locally trained model and metrics to the server\n",
    "                if metrics:\n",
    "                    success = await client.submit_update(model, metrics)\n",
    "                    if not success:\n",
    "                        print(f\"Client {client_id}: Update submission failed.\")\n",
    "                        break\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Client {client_id} encountered an error: {e}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HTTPClient` is a context manager that facilitates communication with the federated server. Using `async with HTTPClient(...)` makes sure that:\n",
    "- The client session is properly opened and closed.\n",
    "- Resources like network connections and memory are managed efficiently.\n",
    "\n",
    "The loop continues until the server signals that the training is complete. The signal is checked using `await client.check_server_status()`.\n",
    "\n",
    "The client starts by fetching the current global model's parameters (`model_state`) from the server:\n",
    "```python\n",
    "model_state, _ = await client.fetch_global_model()\n",
    "```\n",
    "\n",
    "The client uses the global model as a starting point for its local training:\n",
    "```python\n",
    "model = MNISTModel()\n",
    "model.load_state_dict(model_state)\n",
    "```\n",
    "- A new model instance is created to avoid interference with the previous states.\n",
    "- `load_state_dict` initializes the model with the parameters from the global model.\n",
    "\n",
    "This model is then trained on the client's dataset:\n",
    "\n",
    "```python\n",
    "for epoch in range(training_config.epochs):\n",
    "    metrics = trainer.train_epoch(model, train_loader, optimizer, epoch)\n",
    "```\n",
    "- **Metrics** are computed during training and will be sent back to the server along with the updated model.\n",
    "\n",
    "```python\n",
    "success = await client.submit_update(model, metrics)\n",
    "if not success:\n",
    "    break\n",
    "```\n",
    "The server aggregates these updates from multiple clients to update the global model.\n",
    "\n",
    "## Step 8: Running the Federated Experiment\n",
    "\n",
    "Now that the server, coordinator, and client functions are defined, you can run them concurrently to simulate the FL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:28:04,915 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting train_round\n",
      "2024-12-11 21:28:04,916 - coordinator - \u001b[32mINFO\u001b[0m - Client training progress: 0/3 (need 1)\n",
      "2024-12-11 21:28:04,948 - coordinator - \u001b[32mINFO\u001b[0m - Initializing HTTP client for client_1\n",
      "2024-12-11 21:28:04,963 - coordinator - \u001b[32mINFO\u001b[0m - Initializing HTTP client for client_2\n",
      "2024-12-11 21:28:04,976 - coordinator - \u001b[32mINFO\u001b[0m - Initializing HTTP client for client_3\n",
      "2024-12-11 21:28:04,981 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting fetch_global_model\n",
      "2024-12-11 21:28:04,982 - client.http - \u001b[32mINFO\u001b[0m - Fetching global model from http://0.0.0.0:8080/model...\n",
      "2024-12-11 21:28:04,982 - client.http - \u001b[36mDEBUG\u001b[0m - Starting fetch_global_model\n",
      "2024-12-11 21:28:04,982 - client.http - \u001b[32mINFO\u001b[0m - Fetching global model from http://0.0.0.0:8080/model...\n",
      "2024-12-11 21:28:04,983 - client.http - \u001b[36mDEBUG\u001b[0m - Starting fetch_global_model\n",
      "2024-12-11 21:28:04,983 - client.http - \u001b[32mINFO\u001b[0m - Fetching global model from http://0.0.0.0:8080/model...\n",
      "2024-12-11 21:28:06,836 - client.http - \u001b[32mINFO\u001b[0m - Fetched global model.\n",
      "2024-12-11 21:28:06,946 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed fetch_global_model in 1.96s\n",
      "2024-12-11 21:28:06,951 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:28:09,110 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [64/12000 (1%)] Loss: 2.310607 Accuracy: 0.1094\n",
      "2024-12-11 21:28:09,607 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [704/12000 (6%)] Loss: 1.750909 Accuracy: 0.3750\n",
      "2024-12-11 21:28:10,134 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1344/12000 (11%)] Loss: 1.494787 Accuracy: 0.5312\n",
      "2024-12-11 21:28:10,625 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1984/12000 (17%)] Loss: 0.876107 Accuracy: 0.7188\n",
      "2024-12-11 21:28:11,152 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [2624/12000 (22%)] Loss: 0.700368 Accuracy: 0.7656\n",
      "2024-12-11 21:28:11,642 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3264/12000 (27%)] Loss: 0.591887 Accuracy: 0.7500\n",
      "2024-12-11 21:28:12,197 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3904/12000 (33%)] Loss: 0.543896 Accuracy: 0.8281\n",
      "2024-12-11 21:28:12,691 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [4544/12000 (38%)] Loss: 0.510873 Accuracy: 0.8125\n",
      "2024-12-11 21:28:13,182 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [5184/12000 (43%)] Loss: 0.567230 Accuracy: 0.8438\n",
      "2024-12-11 21:28:13,670 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [5824/12000 (49%)] Loss: 0.487078 Accuracy: 0.8438\n",
      "2024-12-11 21:28:14,197 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [6464/12000 (54%)] Loss: 0.560170 Accuracy: 0.8281\n",
      "2024-12-11 21:28:14,689 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [7104/12000 (59%)] Loss: 0.455733 Accuracy: 0.8906\n",
      "2024-12-11 21:28:15,173 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [7744/12000 (65%)] Loss: 0.394330 Accuracy: 0.8750\n",
      "2024-12-11 21:28:15,651 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [8384/12000 (70%)] Loss: 0.363609 Accuracy: 0.9219\n",
      "2024-12-11 21:28:16,174 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [9024/12000 (75%)] Loss: 0.292317 Accuracy: 0.8906\n",
      "2024-12-11 21:28:16,643 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [9664/12000 (81%)] Loss: 0.332136 Accuracy: 0.8750\n",
      "2024-12-11 21:28:17,118 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [10304/12000 (86%)] Loss: 0.256652 Accuracy: 0.9062\n",
      "2024-12-11 21:28:17,598 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [10944/12000 (91%)] Loss: 0.228850 Accuracy: 0.8906\n",
      "2024-12-11 21:28:18,100 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [11584/12000 (97%)] Loss: 0.226667 Accuracy: 0.9219\n",
      "2024-12-11 21:28:18,699 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 11.75s\n",
      "2024-12-11 21:28:18,699 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:28:20,563 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [64/12000 (1%)] Loss: 0.290117 Accuracy: 0.9219\n",
      "2024-12-11 21:28:21,074 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [704/12000 (6%)] Loss: 0.144375 Accuracy: 0.9688\n",
      "2024-12-11 21:28:21,576 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1344/12000 (11%)] Loss: 0.191466 Accuracy: 0.9531\n",
      "2024-12-11 21:28:22,105 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1984/12000 (17%)] Loss: 0.248559 Accuracy: 0.9062\n",
      "2024-12-11 21:28:22,595 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [2624/12000 (22%)] Loss: 0.206055 Accuracy: 0.9531\n",
      "2024-12-11 21:28:23,092 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3264/12000 (27%)] Loss: 0.303927 Accuracy: 0.9219\n",
      "2024-12-11 21:28:23,589 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3904/12000 (33%)] Loss: 0.326795 Accuracy: 0.9375\n",
      "2024-12-11 21:28:24,112 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [4544/12000 (38%)] Loss: 0.377128 Accuracy: 0.8594\n",
      "2024-12-11 21:28:24,607 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [5184/12000 (43%)] Loss: 0.171791 Accuracy: 0.9531\n",
      "2024-12-11 21:28:25,096 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [5824/12000 (49%)] Loss: 0.211140 Accuracy: 0.9375\n",
      "2024-12-11 21:28:25,588 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [6464/12000 (54%)] Loss: 0.198554 Accuracy: 0.9375\n",
      "2024-12-11 21:28:26,071 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [7104/12000 (59%)] Loss: 0.244417 Accuracy: 0.9844\n",
      "2024-12-11 21:28:26,599 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [7744/12000 (65%)] Loss: 0.097118 Accuracy: 0.9688\n",
      "2024-12-11 21:28:27,081 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [8384/12000 (70%)] Loss: 0.054004 Accuracy: 0.9844\n",
      "2024-12-11 21:28:27,538 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [9024/12000 (75%)] Loss: 0.285323 Accuracy: 0.8906\n",
      "2024-12-11 21:28:27,943 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [9664/12000 (81%)] Loss: 0.322883 Accuracy: 0.9375\n",
      "2024-12-11 21:28:28,384 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [10304/12000 (86%)] Loss: 0.143079 Accuracy: 0.9688\n",
      "2024-12-11 21:28:28,788 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [10944/12000 (91%)] Loss: 0.295142 Accuracy: 0.8906\n",
      "2024-12-11 21:28:29,191 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [11584/12000 (97%)] Loss: 0.077992 Accuracy: 0.9688\n",
      "2024-12-11 21:28:29,673 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 10.97s\n",
      "2024-12-11 21:28:29,674 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting submit_update\n",
      "2024-12-11 21:28:29,688 - client.http - \u001b[32mINFO\u001b[0m - Submitting update to http://0.0.0.0:8080/update for round 0\n",
      "2024-12-11 21:28:30,422 - server.http.submit_update - \u001b[32mINFO\u001b[0m - Fetched global model.\n",
      "2024-12-11 21:28:30,530 - client.http - \u001b[36mDEBUG\u001b[0m - Completed fetch_global_model in 25.55s\n",
      "2024-12-11 21:28:30,534 - client.http - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:28:32,339 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [64/8000 (1%)] Loss: 2.292894 Accuracy: 0.1406\n",
      "2024-12-11 21:28:32,801 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [704/8000 (9%)] Loss: 1.939529 Accuracy: 0.3750\n",
      "2024-12-11 21:28:33,266 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1344/8000 (17%)] Loss: 1.438691 Accuracy: 0.5000\n",
      "2024-12-11 21:28:33,722 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1984/8000 (25%)] Loss: 1.111242 Accuracy: 0.6562\n",
      "2024-12-11 21:28:34,176 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [2624/8000 (33%)] Loss: 1.037544 Accuracy: 0.6250\n",
      "2024-12-11 21:28:34,645 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3264/8000 (41%)] Loss: 0.730358 Accuracy: 0.7656\n",
      "2024-12-11 21:28:35,105 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3904/8000 (49%)] Loss: 0.600187 Accuracy: 0.7812\n",
      "2024-12-11 21:28:35,558 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [4544/8000 (57%)] Loss: 0.590491 Accuracy: 0.8281\n",
      "2024-12-11 21:28:35,976 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [5184/8000 (65%)] Loss: 0.619767 Accuracy: 0.7812\n",
      "2024-12-11 21:28:36,425 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [5824/8000 (73%)] Loss: 0.428870 Accuracy: 0.8125\n",
      "2024-12-11 21:28:36,873 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [6464/8000 (81%)] Loss: 0.493978 Accuracy: 0.8906\n",
      "2024-12-11 21:28:37,288 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [7104/8000 (89%)] Loss: 0.665627 Accuracy: 0.8125\n",
      "2024-12-11 21:28:37,704 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [7744/8000 (97%)] Loss: 0.223474 Accuracy: 0.9531\n",
      "2024-12-11 21:28:38,122 - client.http - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 7.59s\n",
      "2024-12-11 21:28:38,122 - client.http - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:28:40,038 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [64/8000 (1%)] Loss: 0.294513 Accuracy: 0.9062\n",
      "2024-12-11 21:28:40,498 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [704/8000 (9%)] Loss: 0.246291 Accuracy: 0.9219\n",
      "2024-12-11 21:28:41,007 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1344/8000 (17%)] Loss: 0.326876 Accuracy: 0.9219\n",
      "2024-12-11 21:28:41,460 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1984/8000 (25%)] Loss: 0.337820 Accuracy: 0.9219\n",
      "2024-12-11 21:28:41,927 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [2624/8000 (33%)] Loss: 0.210390 Accuracy: 0.9375\n",
      "2024-12-11 21:28:42,385 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3264/8000 (41%)] Loss: 0.230358 Accuracy: 0.9062\n",
      "2024-12-11 21:28:42,862 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3904/8000 (49%)] Loss: 0.128104 Accuracy: 0.9688\n",
      "2024-12-11 21:28:43,322 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [4544/8000 (57%)] Loss: 0.214907 Accuracy: 0.9531\n",
      "2024-12-11 21:28:43,773 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [5184/8000 (65%)] Loss: 0.150799 Accuracy: 0.9531\n",
      "2024-12-11 21:28:44,226 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [5824/8000 (73%)] Loss: 0.196063 Accuracy: 0.9531\n",
      "2024-12-11 21:28:44,709 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [6464/8000 (81%)] Loss: 0.171992 Accuracy: 0.9531\n",
      "2024-12-11 21:28:45,166 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [7104/8000 (89%)] Loss: 0.259476 Accuracy: 0.9531\n",
      "2024-12-11 21:28:45,621 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [7744/8000 (97%)] Loss: 0.297697 Accuracy: 0.8750\n",
      "2024-12-11 21:28:46,067 - client.http - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 7.94s\n",
      "2024-12-11 21:28:46,067 - client.http - \u001b[36mDEBUG\u001b[0m - Starting submit_update\n",
      "2024-12-11 21:28:46,081 - client.http - \u001b[32mINFO\u001b[0m - Submitting update to http://0.0.0.0:8080/update for round 0\n",
      "2024-12-11 21:28:46,808 - client.http - \u001b[32mINFO\u001b[0m - Fetched global model.\n",
      "2024-12-11 21:28:46,916 - client.http - \u001b[36mDEBUG\u001b[0m - Completed fetch_global_model in 41.93s\n",
      "2024-12-11 21:28:46,922 - client.http - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:28:48,832 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [64/4000 (2%)] Loss: 2.299017 Accuracy: 0.1094\n",
      "2024-12-11 21:28:49,246 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [704/4000 (18%)] Loss: 1.988632 Accuracy: 0.2969\n",
      "2024-12-11 21:28:49,653 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1344/4000 (34%)] Loss: 1.757729 Accuracy: 0.4375\n",
      "2024-12-11 21:28:50,054 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1984/4000 (50%)] Loss: 0.882082 Accuracy: 0.6719\n",
      "2024-12-11 21:28:50,472 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [2624/4000 (66%)] Loss: 0.765821 Accuracy: 0.7188\n",
      "2024-12-11 21:28:50,937 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3264/4000 (82%)] Loss: 0.509025 Accuracy: 0.8281\n",
      "2024-12-11 21:28:51,410 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3904/4000 (98%)] Loss: 0.687012 Accuracy: 0.7500\n",
      "2024-12-11 21:28:51,733 - client.http - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 4.81s\n",
      "2024-12-11 21:28:51,733 - client.http - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:28:53,540 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [64/4000 (2%)] Loss: 0.695597 Accuracy: 0.7656\n",
      "2024-12-11 21:28:53,992 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [704/4000 (18%)] Loss: 0.447326 Accuracy: 0.9062\n",
      "2024-12-11 21:28:54,444 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1344/4000 (34%)] Loss: 0.670206 Accuracy: 0.7812\n",
      "2024-12-11 21:28:54,920 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1984/4000 (50%)] Loss: 0.389567 Accuracy: 0.8594\n",
      "2024-12-11 21:28:55,372 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [2624/4000 (66%)] Loss: 0.286976 Accuracy: 0.8906\n",
      "2024-12-11 21:28:55,824 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3264/4000 (82%)] Loss: 0.353537 Accuracy: 0.8906\n",
      "2024-12-11 21:28:56,275 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3904/4000 (98%)] Loss: 0.368897 Accuracy: 0.8594\n",
      "2024-12-11 21:28:56,604 - client.http - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 4.87s\n",
      "2024-12-11 21:28:56,604 - client.http - \u001b[36mDEBUG\u001b[0m - Starting submit_update\n",
      "2024-12-11 21:28:56,618 - client.http - \u001b[32mINFO\u001b[0m - Submitting update to http://0.0.0.0:8080/update for round 0\n",
      "2024-12-11 21:28:57,395 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed submit_update in 27.72s\n",
      "2024-12-11 21:28:57,397 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting fetch_global_model\n",
      "2024-12-11 21:28:57,397 - client.http - \u001b[32mINFO\u001b[0m - Fetching global model from http://0.0.0.0:8080/model...\n",
      "2024-12-11 21:28:58,164 - client.http - \u001b[36mDEBUG\u001b[0m - Completed submit_update in 1.56s\n",
      "2024-12-11 21:28:58,165 - client.http - \u001b[32mINFO\u001b[0m - Client training progress: 2/3 (need 1)\n",
      "2024-12-11 21:28:58,165 - client.http - \u001b[32mINFO\u001b[0m - Sufficient clients completed training: 2/3\n",
      "2024-12-11 21:28:58,368 - coordinator.round_0 - \u001b[36mDEBUG\u001b[0m - Client sample counts: [12000, 4000]\n",
      "2024-12-11 21:28:58,368 - coordinator.round_0 - \u001b[36mDEBUG\u001b[0m - Computed weights: [0.75, 0.25]\n",
      "2024-12-11 21:28:58,368 - coordinator.round_0 - \u001b[36mDEBUG\u001b[0m - Starting aggregate\n",
      "2024-12-11 21:28:58,369 - coordinator.round_0 - \u001b[36mDEBUG\u001b[0m - Client sample counts: [12000, 4000]\n",
      "2024-12-11 21:28:58,369 - coordinator.round_0 - \u001b[36mDEBUG\u001b[0m - Computed weights: [0.75, 0.25]\n",
      "2024-12-11 21:28:58,372 - coordinator.round_0 - \u001b[36mDEBUG\u001b[0m - Completed aggregate in 0.00s\n",
      "2024-12-11 21:28:58,372 - coordinator.round_0 - \u001b[36mDEBUG\u001b[0m - Starting save_model\n",
      "2024-12-11 21:28:58,381 - model_manager.save - \u001b[32mINFO\u001b[0m - Saved model version: model_v_20241212_052858_001\n",
      "2024-12-11 21:28:58,381 - coordinator.round_0 - \u001b[36mDEBUG\u001b[0m - Completed save_model in 0.01s\n",
      "2024-12-11 21:28:58,382 - coordinator.metrics.round_0 - \u001b[32mINFO\u001b[0m - Saved metrics for round 0 to runs/metrics/metrics_round_0.json\n",
      "2024-12-11 21:28:58,396 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed train_round in 53.48s\n",
      "2024-12-11 21:28:58,396 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting train_round\n",
      "2024-12-11 21:28:58,396 - coordinator - \u001b[32mINFO\u001b[0m - Client training progress: 0/3 (need 1)\n",
      "2024-12-11 21:28:58,400 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting fetch_global_model\n",
      "2024-12-11 21:28:58,401 - client.http - \u001b[32mINFO\u001b[0m - Fetching global model from http://0.0.0.0:8080/model...\n",
      "2024-12-11 21:28:59,159 - client.http - \u001b[36mDEBUG\u001b[0m - Completed submit_update in 13.09s\n",
      "2024-12-11 21:28:59,161 - client.http - \u001b[36mDEBUG\u001b[0m - Starting fetch_global_model\n",
      "2024-12-11 21:28:59,162 - client.http - \u001b[32mINFO\u001b[0m - Fetching global model from http://0.0.0.0:8080/model...\n",
      "2024-12-11 21:28:59,659 - client.http - \u001b[32mINFO\u001b[0m - Client training progress: 1/3 (need 1)\n",
      "2024-12-11 21:28:59,659 - client.http - \u001b[32mINFO\u001b[0m - Sufficient clients completed training: 1/3\n",
      "2024-12-11 21:28:59,762 - coordinator.round_1 - \u001b[36mDEBUG\u001b[0m - Client sample counts: [8000]\n",
      "2024-12-11 21:28:59,763 - coordinator.round_1 - \u001b[36mDEBUG\u001b[0m - Computed weights: [1.0]\n",
      "2024-12-11 21:28:59,763 - coordinator.round_1 - \u001b[36mDEBUG\u001b[0m - Starting aggregate\n",
      "2024-12-11 21:28:59,763 - coordinator.round_1 - \u001b[36mDEBUG\u001b[0m - Client sample counts: [8000]\n",
      "2024-12-11 21:28:59,763 - coordinator.round_1 - \u001b[36mDEBUG\u001b[0m - Computed weights: [1.0]\n",
      "2024-12-11 21:28:59,765 - coordinator.round_1 - \u001b[36mDEBUG\u001b[0m - Completed aggregate in 0.00s\n",
      "2024-12-11 21:28:59,765 - coordinator.round_1 - \u001b[36mDEBUG\u001b[0m - Starting save_model\n",
      "2024-12-11 21:28:59,773 - model_manager.save - \u001b[32mINFO\u001b[0m - Saved model version: model_v_20241212_052859_002\n",
      "2024-12-11 21:28:59,774 - coordinator.round_1 - \u001b[36mDEBUG\u001b[0m - Completed save_model in 0.01s\n",
      "2024-12-11 21:28:59,775 - coordinator.metrics.round_1 - \u001b[32mINFO\u001b[0m - Saved metrics for round 1 to runs/metrics/metrics_round_1.json\n",
      "2024-12-11 21:28:59,782 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed train_round in 1.39s\n",
      "2024-12-11 21:28:59,783 - coordinator - \u001b[32mINFO\u001b[0m - Training completed. Broadcasting termination signal to clients.\n",
      "2024-12-11 21:28:59,783 - coordinator - \u001b[32mINFO\u001b[0m - Training completed\n",
      "2024-12-11 21:28:59,783 - coordinator.run - \u001b[32mINFO\u001b[0m - Coordinator run completed.\n",
      "2024-12-11 21:29:00,062 - nanofed - \u001b[32mINFO\u001b[0m - Fetched global model.\n",
      "2024-12-11 21:29:00,171 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed fetch_global_model in 1.77s\n",
      "2024-12-11 21:29:00,176 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:29:02,082 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [64/4000 (2%)] Loss: 0.105592 Accuracy: 0.9844\n",
      "2024-12-11 21:29:02,640 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [704/4000 (18%)] Loss: 0.132262 Accuracy: 0.9688\n",
      "2024-12-11 21:29:03,165 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1344/4000 (34%)] Loss: 0.154564 Accuracy: 0.9531\n",
      "2024-12-11 21:29:03,656 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1984/4000 (50%)] Loss: 0.072375 Accuracy: 0.9844\n",
      "2024-12-11 21:29:04,150 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [2624/4000 (66%)] Loss: 0.235519 Accuracy: 0.9219\n",
      "2024-12-11 21:29:04,614 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3264/4000 (82%)] Loss: 0.168337 Accuracy: 0.9688\n",
      "2024-12-11 21:29:05,143 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3904/4000 (98%)] Loss: 0.191436 Accuracy: 0.9531\n",
      "2024-12-11 21:29:05,506 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 5.33s\n",
      "2024-12-11 21:29:05,506 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:29:07,424 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [64/4000 (2%)] Loss: 0.129293 Accuracy: 0.9688\n",
      "2024-12-11 21:29:07,897 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [704/4000 (18%)] Loss: 0.162178 Accuracy: 0.9062\n",
      "2024-12-11 21:29:08,380 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1344/4000 (34%)] Loss: 0.097537 Accuracy: 0.9531\n",
      "2024-12-11 21:29:08,856 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1984/4000 (50%)] Loss: 0.125252 Accuracy: 0.9688\n",
      "2024-12-11 21:29:09,430 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [2624/4000 (66%)] Loss: 0.239187 Accuracy: 0.9531\n",
      "2024-12-11 21:29:09,912 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3264/4000 (82%)] Loss: 0.274475 Accuracy: 0.9375\n",
      "2024-12-11 21:29:10,440 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3904/4000 (98%)] Loss: 0.055113 Accuracy: 0.9844\n",
      "2024-12-11 21:29:10,814 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 5.31s\n",
      "2024-12-11 21:29:10,815 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting submit_update\n",
      "2024-12-11 21:29:10,828 - client.http - \u001b[32mINFO\u001b[0m - Submitting update to http://0.0.0.0:8080/update for round 0\n",
      "2024-12-11 21:29:11,563 - server.http.submit_update - \u001b[32mINFO\u001b[0m - Fetched global model.\n",
      "2024-12-11 21:29:11,671 - client.http - \u001b[36mDEBUG\u001b[0m - Completed fetch_global_model in 12.51s\n",
      "2024-12-11 21:29:11,677 - client.http - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:29:13,528 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [64/8000 (1%)] Loss: 0.230687 Accuracy: 0.9531\n",
      "2024-12-11 21:29:13,984 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [704/8000 (9%)] Loss: 0.091289 Accuracy: 0.9688\n",
      "2024-12-11 21:29:14,435 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1344/8000 (17%)] Loss: 0.233721 Accuracy: 0.9375\n",
      "2024-12-11 21:29:14,937 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1984/8000 (25%)] Loss: 0.249253 Accuracy: 0.9219\n",
      "2024-12-11 21:29:15,471 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [2624/8000 (33%)] Loss: 0.188620 Accuracy: 0.9375\n",
      "2024-12-11 21:29:15,955 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3264/8000 (41%)] Loss: 0.226967 Accuracy: 0.9531\n",
      "2024-12-11 21:29:16,444 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3904/8000 (49%)] Loss: 0.210711 Accuracy: 0.9375\n",
      "2024-12-11 21:29:16,996 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [4544/8000 (57%)] Loss: 0.043493 Accuracy: 0.9844\n",
      "2024-12-11 21:29:17,530 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [5184/8000 (65%)] Loss: 0.236407 Accuracy: 0.9375\n",
      "2024-12-11 21:29:17,942 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [5824/8000 (73%)] Loss: 0.204998 Accuracy: 0.9219\n",
      "2024-12-11 21:29:18,352 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [6464/8000 (81%)] Loss: 0.181957 Accuracy: 0.9688\n",
      "2024-12-11 21:29:18,757 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [7104/8000 (89%)] Loss: 0.260201 Accuracy: 0.9219\n",
      "2024-12-11 21:29:19,162 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [7744/8000 (97%)] Loss: 0.141820 Accuracy: 0.9531\n",
      "2024-12-11 21:29:19,551 - client.http - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 7.87s\n",
      "2024-12-11 21:29:19,551 - client.http - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:29:21,454 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [64/8000 (1%)] Loss: 0.090219 Accuracy: 0.9688\n",
      "2024-12-11 21:29:21,866 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [704/8000 (9%)] Loss: 0.062645 Accuracy: 0.9844\n",
      "2024-12-11 21:29:22,272 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1344/8000 (17%)] Loss: 0.164013 Accuracy: 0.9375\n",
      "2024-12-11 21:29:22,778 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1984/8000 (25%)] Loss: 0.238593 Accuracy: 0.9219\n",
      "2024-12-11 21:29:23,236 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [2624/8000 (33%)] Loss: 0.209369 Accuracy: 0.9375\n",
      "2024-12-11 21:29:23,711 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3264/8000 (41%)] Loss: 0.074389 Accuracy: 0.9844\n",
      "2024-12-11 21:29:24,188 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3904/8000 (49%)] Loss: 0.061021 Accuracy: 1.0000\n",
      "2024-12-11 21:29:24,677 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [4544/8000 (57%)] Loss: 0.137467 Accuracy: 0.9375\n",
      "2024-12-11 21:29:25,179 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [5184/8000 (65%)] Loss: 0.252423 Accuracy: 0.9375\n",
      "2024-12-11 21:29:25,705 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [5824/8000 (73%)] Loss: 0.098408 Accuracy: 0.9688\n",
      "2024-12-11 21:29:26,205 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [6464/8000 (81%)] Loss: 0.088927 Accuracy: 0.9688\n",
      "2024-12-11 21:29:26,707 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [7104/8000 (89%)] Loss: 0.074699 Accuracy: 0.9844\n",
      "2024-12-11 21:29:27,219 - client.http - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [7744/8000 (97%)] Loss: 0.146289 Accuracy: 0.9531\n",
      "2024-12-11 21:29:27,734 - client.http - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 8.18s\n",
      "2024-12-11 21:29:27,735 - client.http - \u001b[36mDEBUG\u001b[0m - Starting submit_update\n",
      "2024-12-11 21:29:27,748 - client.http - \u001b[32mINFO\u001b[0m - Submitting update to http://0.0.0.0:8080/update for round 0\n",
      "2024-12-11 21:29:28,475 - client.http - \u001b[32mINFO\u001b[0m - Fetched global model.\n",
      "2024-12-11 21:29:28,585 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed fetch_global_model in 31.19s\n",
      "2024-12-11 21:29:28,590 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:29:30,453 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [64/12000 (1%)] Loss: 2.317849 Accuracy: 0.0781\n",
      "2024-12-11 21:29:30,947 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [704/12000 (6%)] Loss: 1.749919 Accuracy: 0.4688\n",
      "2024-12-11 21:29:31,439 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1344/12000 (11%)] Loss: 1.449968 Accuracy: 0.5156\n",
      "2024-12-11 21:29:31,956 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [1984/12000 (17%)] Loss: 1.077645 Accuracy: 0.6875\n",
      "2024-12-11 21:29:32,441 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [2624/12000 (22%)] Loss: 0.743921 Accuracy: 0.7344\n",
      "2024-12-11 21:29:32,951 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3264/12000 (27%)] Loss: 1.048301 Accuracy: 0.6719\n",
      "2024-12-11 21:29:33,507 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [3904/12000 (33%)] Loss: 0.522212 Accuracy: 0.8281\n",
      "2024-12-11 21:29:34,044 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [4544/12000 (38%)] Loss: 0.649267 Accuracy: 0.7812\n",
      "2024-12-11 21:29:34,554 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [5184/12000 (43%)] Loss: 0.542364 Accuracy: 0.9062\n",
      "2024-12-11 21:29:35,056 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [5824/12000 (49%)] Loss: 0.429820 Accuracy: 0.9062\n",
      "2024-12-11 21:29:35,557 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [6464/12000 (54%)] Loss: 0.398396 Accuracy: 0.8906\n",
      "2024-12-11 21:29:36,106 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [7104/12000 (59%)] Loss: 0.252711 Accuracy: 0.9219\n",
      "2024-12-11 21:29:36,606 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [7744/12000 (65%)] Loss: 0.463154 Accuracy: 0.8281\n",
      "2024-12-11 21:29:37,100 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [8384/12000 (70%)] Loss: 0.279286 Accuracy: 0.9219\n",
      "2024-12-11 21:29:37,593 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [9024/12000 (75%)] Loss: 0.279347 Accuracy: 0.9062\n",
      "2024-12-11 21:29:38,125 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [9664/12000 (81%)] Loss: 0.284247 Accuracy: 0.9219\n",
      "2024-12-11 21:29:38,637 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [10304/12000 (86%)] Loss: 0.389055 Accuracy: 0.8750\n",
      "2024-12-11 21:29:39,143 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [10944/12000 (91%)] Loss: 0.263091 Accuracy: 0.9375\n",
      "2024-12-11 21:29:39,649 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 0 [11584/12000 (97%)] Loss: 0.373842 Accuracy: 0.9062\n",
      "2024-12-11 21:29:40,319 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 11.73s\n",
      "2024-12-11 21:29:40,319 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting train_epoch\n",
      "2024-12-11 21:29:42,185 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [64/12000 (1%)] Loss: 0.333607 Accuracy: 0.8750\n",
      "2024-12-11 21:29:42,667 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [704/12000 (6%)] Loss: 0.412039 Accuracy: 0.8594\n",
      "2024-12-11 21:29:43,154 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1344/12000 (11%)] Loss: 0.552476 Accuracy: 0.8281\n",
      "2024-12-11 21:29:43,728 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [1984/12000 (17%)] Loss: 0.283576 Accuracy: 0.9219\n",
      "2024-12-11 21:29:44,251 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [2624/12000 (22%)] Loss: 0.413386 Accuracy: 0.9375\n",
      "2024-12-11 21:29:44,749 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3264/12000 (27%)] Loss: 0.311194 Accuracy: 0.9219\n",
      "2024-12-11 21:29:45,272 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [3904/12000 (33%)] Loss: 0.169683 Accuracy: 0.9688\n",
      "2024-12-11 21:29:45,780 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [4544/12000 (38%)] Loss: 0.194807 Accuracy: 0.9375\n",
      "2024-12-11 21:29:46,246 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [5184/12000 (43%)] Loss: 0.163947 Accuracy: 0.9062\n",
      "2024-12-11 21:29:46,692 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [5824/12000 (49%)] Loss: 0.321606 Accuracy: 0.8906\n",
      "2024-12-11 21:29:47,137 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [6464/12000 (54%)] Loss: 0.074207 Accuracy: 0.9844\n",
      "2024-12-11 21:29:47,583 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [7104/12000 (59%)] Loss: 0.266417 Accuracy: 0.9062\n",
      "2024-12-11 21:29:48,056 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [7744/12000 (65%)] Loss: 0.148687 Accuracy: 0.9531\n",
      "2024-12-11 21:29:48,511 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [8384/12000 (70%)] Loss: 0.130994 Accuracy: 0.9688\n",
      "2024-12-11 21:29:48,952 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [9024/12000 (75%)] Loss: 0.115552 Accuracy: 0.9531\n",
      "2024-12-11 21:29:49,370 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [9664/12000 (81%)] Loss: 0.126552 Accuracy: 0.9531\n",
      "2024-12-11 21:29:49,802 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [10304/12000 (86%)] Loss: 0.191542 Accuracy: 0.9375\n",
      "2024-12-11 21:29:50,256 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [10944/12000 (91%)] Loss: 0.176815 Accuracy: 0.9531\n",
      "2024-12-11 21:29:50,683 - coordinator - \u001b[32mINFO\u001b[0m - Train Epoch: 1 [11584/12000 (97%)] Loss: 0.287119 Accuracy: 0.9062\n",
      "2024-12-11 21:29:51,198 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed train_epoch in 10.88s\n",
      "2024-12-11 21:29:51,199 - coordinator - \u001b[36mDEBUG\u001b[0m - Starting submit_update\n",
      "2024-12-11 21:29:51,213 - client.http - \u001b[32mINFO\u001b[0m - Submitting update to http://0.0.0.0:8080/update for round 0\n",
      "2024-12-11 21:29:51,992 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed submit_update in 41.18s\n",
      "2024-12-11 21:29:51,994 - coordinator - \u001b[32mINFO\u001b[0m - Closing HTTP client for client_3\n",
      "2024-12-11 21:29:52,264 - client.http - \u001b[36mDEBUG\u001b[0m - Completed submit_update in 24.53s\n",
      "2024-12-11 21:29:52,526 - coordinator - \u001b[36mDEBUG\u001b[0m - Completed submit_update in 1.33s\n",
      "2024-12-11 21:29:52,527 - coordinator - \u001b[32mINFO\u001b[0m - Closing HTTP client for client_2\n",
      "2024-12-11 21:29:52,529 - coordinator - \u001b[32mINFO\u001b[0m - Closing HTTP client for client_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 21:28:04,980 - coordinator - \u001b[32mINFO\u001b[0m - Processing /status request.\n",
      "2024-12-11 21:28:04,980 - coordinator - \u001b[32mINFO\u001b[0m - Processing /status request.\n",
      "2024-12-11 21:28:04,981 - coordinator - \u001b[32mINFO\u001b[0m - Processing /status request.\n",
      "2024-12-11 21:28:04,983 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Processing /model request.\n",
      "2024-12-11 21:28:04,984 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Starting load_model\n",
      "2024-12-11 21:28:04,995 - model_manager.load - \u001b[32mINFO\u001b[0m - Loaded model version: model_v_20241212_052409_001\n",
      "2024-12-11 21:28:04,996 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Completed load_model in 0.01s\n",
      "2024-12-11 21:28:04,996 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Serving model version model_v_20241212_052409_001\n",
      "2024-12-11 21:28:05,012 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Model response prepared for version model_v_20241212_052409_001\n",
      "2024-12-11 21:28:05,501 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Processing /model request.\n",
      "2024-12-11 21:28:05,501 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Serving model version model_v_20241212_052409_001\n",
      "2024-12-11 21:28:05,578 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Model response prepared for version model_v_20241212_052409_001\n",
      "2024-12-11 21:28:06,066 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Processing /model request.\n",
      "2024-12-11 21:28:06,066 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Serving model version model_v_20241212_052409_001\n",
      "2024-12-11 21:28:06,079 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Model response prepared for version model_v_20241212_052409_001\n",
      "2024-12-11 21:28:30,168 - server.http.submit_update - \u001b[36mDEBUG\u001b[0m - Processing /update request.\n",
      "2024-12-11 21:28:57,090 - server.http.submit_update - \u001b[36mDEBUG\u001b[0m - Processing /update request.\n",
      "2024-12-11 21:28:57,093 - server.http.submit_update - \u001b[36mDEBUG\u001b[0m - Processing /update request.\n",
      "2024-12-11 21:28:57,383 - server.http.submit_update - \u001b[32mINFO\u001b[0m - Accepted update from client client_1 for round in 0\n",
      "2024-12-11 21:28:57,396 - coordinator - \u001b[32mINFO\u001b[0m - Processing /status request.\n",
      "2024-12-11 21:28:57,398 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Processing /model request.\n",
      "2024-12-11 21:28:57,399 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Serving model version model_v_20241212_052409_001\n",
      "2024-12-11 21:28:57,412 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Model response prepared for version model_v_20241212_052409_001\n",
      "2024-12-11 21:28:58,156 - client.http - \u001b[32mINFO\u001b[0m - Accepted update from client client_3 for round in 0\n",
      "2024-12-11 21:28:58,398 - coordinator - \u001b[32mINFO\u001b[0m - Processing /status request.\n",
      "2024-12-11 21:28:58,402 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Processing /model request.\n",
      "2024-12-11 21:28:58,402 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Serving model version model_v_20241212_052858_001\n",
      "2024-12-11 21:28:58,415 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Model response prepared for version model_v_20241212_052858_001\n",
      "2024-12-11 21:28:59,147 - client.http - \u001b[32mINFO\u001b[0m - Accepted update from client client_2 for round in 0\n",
      "2024-12-11 21:28:59,161 - client.http - \u001b[32mINFO\u001b[0m - Processing /status request.\n",
      "2024-12-11 21:28:59,162 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Processing /model request.\n",
      "2024-12-11 21:28:59,163 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Serving model version model_v_20241212_052858_001\n",
      "2024-12-11 21:28:59,177 - server.http.get_model - \u001b[36mDEBUG\u001b[0m - Model response prepared for version model_v_20241212_052858_001\n",
      "2024-12-11 21:29:11,307 - server.http.submit_update - \u001b[36mDEBUG\u001b[0m - Processing /update request.\n",
      "2024-12-11 21:29:51,695 - server.http.submit_update - \u001b[36mDEBUG\u001b[0m - Processing /update request.\n",
      "2024-12-11 21:29:51,696 - server.http.submit_update - \u001b[36mDEBUG\u001b[0m - Processing /update request.\n",
      "2024-12-11 21:29:51,981 - server.http.submit_update - \u001b[32mINFO\u001b[0m - Accepted update from client client_3 for round in 0\n",
      "2024-12-11 21:29:51,993 - coordinator - \u001b[32mINFO\u001b[0m - Processing /status request.\n",
      "2024-12-11 21:29:52,255 - coordinator - \u001b[32mINFO\u001b[0m - Accepted update from client client_2 for round in 0\n",
      "2024-12-11 21:29:52,518 - client.http - \u001b[32mINFO\u001b[0m - Accepted update from client client_1 for round in 0\n",
      "2024-12-11 21:29:52,519 - server.http.submit_update - \u001b[32mINFO\u001b[0m - Processing /status request.\n",
      "2024-12-11 21:29:52,529 - coordinator - \u001b[32mINFO\u001b[0m - Processing /status request.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await asyncio.gather(\n",
    "    coordinate(coordinator),\n",
    "    run_client(\"client_1\", coordinator, num_samples=12000),\n",
    "    run_client(\"client_2\", coordinator, num_samples=8000),\n",
    "    run_client(\"client_3\", coordinator, num_samples=4000),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the federated learning experiment, NanoFed generates several outputs, organizes them into directories, and provides detailed logs and saved artifacts.\n",
    "\n",
    "1. `runs/models/`:\n",
    "    - Subdirectory for storing global model checkpoints.\n",
    "    - `configs/`:\n",
    "        - Stores metadata and configuration files for each saved model version.\n",
    "    - `models/`:\n",
    "        - Stores serialized PyTorch model files.\n",
    "2. `runs/metrics/`:\n",
    "    - Stores JSON files contianing aggregated metrics for each training round.\n",
    "3. `runs/data/`:\n",
    "    - (Optional) A subdirectory for client-specific datasets or any intermediate data.\n",
    "\n",
    "### Example Metrics Artifact\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"round_id\": 1,\n",
    "    \"start_time\": \"2024-12-12T05:28:58.396750+00:00\",\n",
    "    \"end_time\": \"2024-12-12T05:28:59.774794+00:00\",\n",
    "    \"num_clients\": 1,\n",
    "    \"agg_metrics\": {\n",
    "        \"loss\": 0.25233903527259827,\n",
    "        \"accuracy\": 0.9375,\n",
    "        \"samples_processed\": 8000.0\n",
    "    },\n",
    "    \"status\": \"COMPLETED\",\n",
    "    \"client_metrics\": [\n",
    "        {\n",
    "            \"client_id\": \"client_2\",\n",
    "            \"metrics\": {\n",
    "                \"loss\": 0.25233903527259827,\n",
    "                \"accuracy\": 0.9375,\n",
    "                \"samples_processed\": 8000\n",
    "            },\n",
    "            \"weight\": 1.0\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "#### Top-Level Fields\n",
    "1. `round_id`: Identifier for the training round (i.e., `1` for the second round).\n",
    "2. `start_time`/`end_time`: ISO 8601 timestamps marking the round's start and end.\n",
    "3. `num_clients`: Number of clients that successfully submitted updates (i.e., `2`).\n",
    "4. `agg_metrics`: Weighted aggregation metrics across clients.\n",
    "5. `status`: Outcome of the round.\n",
    "\n",
    "#### Client-Specific Metrics\n",
    "1. `client_id`: Identifier for the client.\n",
    "2. `metrics`: Local metrics reported by the client's local training.\n",
    "3. `weight`: Proportional contribution of the client to the global model. In FedAvg, $\\text{weight} = \\frac{\\text{client samples}}{\\text{total samples}}$\n",
    "\n",
    "Note that the field `num_clients` shows that only **1 client** contributed to the round. This behavior is determined by the `min_completion_rate` configuration, which controls the minimum number of clients required to submit updates for the round to complete successfully. More clients can contribute to a training round.\n",
    "\n",
    "As we specified `min_clients` to be `3`, 3 clients must still participate in the trianing process, but since `min_completion_rate` is `0.5` in this example,\n",
    "\n",
    "$$\n",
    "\\text{required clients}= \\text{floor}(\\text{min clients} \\times \\text{min completion rate}) = \\text{floor}(3 \\times 0.5)=1\n",
    "$$\n",
    "\n",
    "**1** client is required to submit an update.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "You have successfully completed a federated learning experiment using NanoFed. This tutorial demonstrated how to:\n",
    "1. Set up the global model and federated server.\n",
    "2. Configure the training coordinator and aggregation strategy.\n",
    "3. Implement and manage federated clients.\n",
    "4. Run the experiment and analyze the generated results.\n",
    "\n",
    "Feel free to experiment with different configurations, such as:\n",
    "- Changing the number of clients and completion rates.\n",
    "- Extending the `BaseAggregator` to implement custom aggregation strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanofed-P5zfH5fx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
